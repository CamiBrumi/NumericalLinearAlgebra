\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Apt 1.3, Pràctica 3 ALN: Mínims quadrats}
\author{Camelia Brumar \\ Leonor Lamsdorff-Galagane}
\date{08 May 2017}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[a4paper, total={6.5in, 10in}]{geometry}

\begin{document}

\maketitle

\section{Demostració que $p_{M}(x)$ és el polinomi “òptim”}

Sigui $E$ un espai euclidià, i $F\subseteq E$ un sub-espai vectorial de dimensió finita. Procedirem a provar que, $\forall x \in E$ es té:
$$ \| x - \pi_F\left(x\right) \| < \| x - u \|, \hspace{0.5cm} \forall u \in F, \hspace{0.5cm} u \ne \pi_F\left(x\right) $$

És a dir, que la projecció ortogonal $\pi_F\left(x\right)$ és la millor aproximació de $x$ en $F$.

$$ x - u = \left( x - \pi_F\left(x\right) \right) + \left( \pi_F\left(x\right) - u \right) $$

Com els dos sumands de la dreta són ortogonals (el primer pertany a $F^\perp$ i el segon a $F$), s'aplica el Teorema de Pitàgores: 

$$ \| x - u\|^2 = \| x - \pi_F\left(x\right) \|^2 + \| \pi_F\left(x\right) - u \|^2$$

Posat que $ u \ne \pi_F\left(x\right) $  i  $\| \pi_F\left(x\right) - u \|^2 > 0$, es té:

$$  \| x - u\|^2 > \| x - \pi_F\left(x\right) \|^2  $$

Com la norma és un nombre real no negatiu, ja hem acabat.
Una vegada provat aquest resultat, cal veure quina forma prenen els vectors $ x \in M_{n\times 1} \left( \mathbb{R} \right) $ que minimitzen la norma $ \| Ax - b \| $, on $A\in M_{m\times n} \left( \mathbb{R} \right) $ i $ b \in M_{m\times 1} \left( \mathbb{R} \right)$, i veurem que són  les solucions del sistema

$$ A^\top A x = A^\top b $$

Si prenem $ F = [u_1, ... , u_n ] $ com el sev generat per les columnes de A, observem que $ Ax $ són combinacions lineals de les columnes de A, és a dir, $\{  Ax \}_{x \in M_{n\times 1} \left( \mathbb{R} \right)}\in F$.

$x^*$  minimitza la norma $ \| Ax - b \| \iff Ax^* = \pi_F\left(b\right)$, pel resultat anterior. Però a la vegada, $ Ax^* = \pi_F\left(b\right) \iff <u_i, Ax^*> \hspace{0.2cm} =  \hspace{0.2cm}<u_i,b> \forall i = 1\div n$ (caracterització coneguda de la projecció ortogonal). Per últim, $$  <u_i, Ax^*> \hspace{0.2cm} =  \hspace{0.2cm}<u_i,b> \forall i = 1\div n \iff A^\top Ax^*= A^\top b $$


\textbf{Conclusió}: les solucions del sistema $  A^\top Ax= A^\top b $ minimitzen la norma $ \| Ax - b \|. \hfill \square$ \newline

Aleshores, si volem trobar un polinomi de grau $m$ que aproximi  uns vectors de punts $ x = \left(x_0,...,x_n\right) $ i $y = \left(y_0...,y_n\right)$, estem buscant un polinomi $a_0x^m+a_1x^{m-1}+...+a_{m-1}x^1+a_{m}$ que satisfaci:
$$ a_0x_{0}^{m}+a_1x_{0}^{m-1}+...+a_{m-1}x_{0}^1+a_{m} = y_0 $$
$$ a_0x_{1}^{m}+a_1x_{1}^{m-1}+...+a_{m-1}x_{1}^1+a_{m} = y_1 $$
$$...$$
$$ a_0x_{n}^{m}+a_1x_{n}^{m-1}+...+a_{m-1}x_{n}^1+a_{m} = y_n $$

Matricialment, equival a resoldre el sistema

$$
\label{def:matriu:A}
\left(
\begin{array}{ccccccc}
x_{0}^{m}   &  x_{0}^{m-1}  &   \dots    & x_{0}^1        &    1 \\
x_{1}^{m}   &  x_{1}^{m-1}  &   \dots    & x_{1}^1        &    1 \\
 \vdots  &  \vdots  &  \vdots   &     \vdots    &     \vdots \\
x_{n}^{m}   &  x_{n}^{m-1}  &   \dots    & x_{n}^1        &    1 \\
\end{array}
\right)
\left(
\begin{array}{ccccccc}
a_{0} \\
a_{1} \\
 \vdots \\
a_{m}  \\
\end{array}
\right)=
\left(
\begin{array}{ccccccc}
y_{0} \\
y_{1} \\
 \vdots \\
y_{n}  \\
\end{array}
\right)
$$

Com, en general, $m < n$, no hi ha solució, però gràcies als apartats anteriors sabem que el polinomi $a_0^*x^m+a_1^*x^{m-1}+...+a_{m-1}^*x^1+a_{m}^*$ obtingut com a solució de les equacions normals és el que minimitza la norma $ \| Ax - b \|$.

\section{Justificació del mètode}
A continuació justificarem el mètode utilitzat per trobar les solucions de les equacions normals i l'explicarem breument.

En aquesta pràctica hem utilitzat els mètodes d'ortogonalització, que aconsegueixen la reducció a forma triangular mitjançant l'ús de matrius ortogonals. Aquests mètodes estan basats en la descomposició $QR$ de matrius. Hem triat aquests mètodes perquè respecte d'altres, com Cholesky (si la matriu A és simètrica) o $LU$, presenta certs avatatges. Per una banda, en fer Cholesky, s'haurien de resoldre dos sistemes lineals (a més d'altres càlculs anteriors), mentre que en fer $QR$ només hem de resoldre un sistema triangular superior. Per altra banda, en comparar la $QR$ amb la $LU$, la primera és molt més estable en quant als errors respecte la segona, tot i que és més ineficient pel que fa el temps computacional.

Com en els mètodes gaussians, s'observen dues parts en el procés d'ortogonalització:

\begin{itemize}
  \item En la primera, es factoritza A en la forma:
  $$ A =  QR, $$
  on $Q$ és ortogonal i $R$ triangular superior.
  \item En la segona, s'acaba la resolució del sistema $Ax = QRx = b$, resolent el sistema triangular superior $Rx = Q^\top b$ pel mètode de substitució cap enrere. Arribem a aquest sistema senzill seguint els següents passos: 
  $$A^\top Ax = A^\top b \;,$$
  com que $A = QR$
  $$\left(QR \right)^\top QRx = \left(QR \right)^\top b$$
  $$R^\top Q^\top  QRx = R^\top Q^\top  b$$
  sabent que $Q$ és una matriu ortogonal, i.e, $Q^\top =\ Q^{-1}$, obtenim
  $$R^\top Rx = R^\top Q^\top  b$$
  i finalment multipliquem per la inversa de $R^\top$ a les dues bandes de la igualtat, aconseguint el sistema
  $$Rx = Q^\top b \; .$$
\end{itemize}
Aquesta factorització l'hem realitzat amb el mètode d'ortogonalització mo\-di\-fi\-cat de Gram-Schmidt.\newline
\newline
\textbf{Mètode d'ortogonalització modificat de Gram-Schmidt}\newline
\newline
Començant amb $A_{1} = A$, una vegada coneguda
$$A_{k} = \left(q_{1}\: \ldots{}\: q_{k-1}\: a_{k}^{(k)}\: \ldots{} \:a_{n}^{(k)} \right)$$
amb columnes $q_{j}\;\left(j = 1 \div k - 1  \right)$ i $a_{s}^{(k)}\, \left(s = k \div n \right)$ complint amb les relacions d'ortogonalitat
$$q_{j}^\top q_{l} = \delta_{jl},\; q_{j}^\top a_{s} = 0\, \left(j, l = 1 \div k-1,\, s = k \div n \right),$$
es normalitza la $k$-èssima columna i s'ortogonalitzen, respecte a ella, totes les que la segueixen:
$$r_{kk} = \left| \left|\; a_{k}^{(k)}\; \right| \right|_{2}, \; q_{k} = \frac{a_{k}^{(k)}}{r_{kk}}\;; $$ 
$$r_{ks} = q_{k}^\top a_{s}^{(k)}\; , \; \; a_{s}^{(k+1)} = a_{s}^{(k)} - r_{ks}q_{k} \; \; \left( s = k+1 \div n \right) .$$
S'obté
$$A_{k+1} = \left( q_{1} \; \ldots{} \; q_{k} \; a_{k+1}^{(k+1)} \; \ldots{} \; a_{n}^{(k+1)} \right) \; ,$$

verificant les mateixes condicions d'ortogonalitat anteriors, substituint $k$ per $k+1 \; \left( k = 1 \div n \right).$ \newline
Després de $n$ passos,
$$A_{n+1} = \left(q_{1} \; q_{2} \ldots{} q_{n} \right)$$
és una matriu ortogonal perquè $q_{j}^{T}q_{l} = \delta_{jl} \; \left( j, l = 1 \div n \right).$ \newline
\newline
Les matrius $Q = A_{n+1}$ i $R = r_{(ks)}$ formen una factorització $QR$ de $A$.
\end{document}